{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54254a5",
   "metadata": {},
   "source": [
    "# KNN Model Exercises\n",
    "\n",
    "1. Fit a K-Nearest Neighbors classifier to your training sample and transform (i.e. make predictions on the training sample)\n",
    "\n",
    "2. Evaluate your results using the model score, confusion matrix, and classification report.\n",
    "\n",
    "3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support.\n",
    "\n",
    "4. Run through steps 1-3 setting k to 10\n",
    "\n",
    "5. Run through steps 1-3 setting k to 20\n",
    "\n",
    "6. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?\n",
    "\n",
    "7. Which model performs best on our out-of-sample data from validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "605f0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "\n",
    "# ds libs for tab data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# knn modules - models\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# data to use\n",
    "from acquire import get_titanic_data\n",
    "from prepare import prep_titanic, split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf55ff0",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0c7ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data and stacking all of the changes on top of each other\n",
    "\n",
    "train, validate, test = split_data(prep_titanic(get_titanic_data()), target = 'survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85a02e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in train if train[col].dtype != 'O']\n",
    "features.remove('survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f70ef44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning feature\n",
    "X_train = train[features]\n",
    "X_val = validate[features]\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc46abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning targets\n",
    "y_train = train.survived\n",
    "y_val = validate.survived\n",
    "y_test = test.survived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5af26",
   "metadata": {},
   "source": [
    "### Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7803014",
   "metadata": {},
   "source": [
    "> #### 1. Fit a K-Nearest Neighbors classifier to your training sample and transform (i.e. make predictions on the training sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beb8e0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# creating\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# fiting\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# use\n",
    "y_pred = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4275a",
   "metadata": {},
   "source": [
    "> #### 2. Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1df57268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.785140562248996"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c4a0ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[264,  43],\n",
       "       [ 64, 127]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0671c26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0       0    1\n",
       "survived          \n",
       "0         264   43\n",
       "1          64  127"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crosstab so I can make sense of the sonfucion matrix\n",
    "pd.crosstab(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a33c434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       307\n",
      "           1       0.75      0.66      0.70       191\n",
      "\n",
      "    accuracy                           0.79       498\n",
      "   macro avg       0.78      0.76      0.77       498\n",
      "weighted avg       0.78      0.79      0.78       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c8b6a",
   "metadata": {},
   "source": [
    "> #### 3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72d6099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25497e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp + tn) / (tp + tn + fn + fp)\n",
    "\n",
    "tp_rate =  tp / (tp + fn)\n",
    "\n",
    "fp_rate = fp / (fp + tn)\n",
    "\n",
    "tn_rate = tn / (tn + fp)\n",
    "\n",
    "fn_rate = fn / (tp + fn)\n",
    "\n",
    "precision_ = tp / (tp + fp)\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "f1 = 2 * (precision_ * recall) / (precision_ + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "396e5acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       \n",
      "       Model: KNeighborsClassifier()\n",
      "       Accuracy: 78.51%\n",
      "       True Postive Rate: 66.49%\n",
      "       False Positive Rate: 14.01%\n",
      "       True Negative Rate: 85.99%\n",
      "       False Negative Rate: 33.51%\n",
      "       Precision: 74.71%\n",
      "       Recall: 66.49%\n",
      "       F1: 70.36%\n",
      "       Validation Set Classification Report:\n",
      "       \n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       307\n",
      "           1       0.75      0.66      0.70       191\n",
      "\n",
      "    accuracy                           0.79       498\n",
      "   macro avg       0.78      0.76      0.77       498\n",
      "weighted avg       0.78      0.79      0.78       498\n",
      "\n",
      "       \n"
     ]
    }
   ],
   "source": [
    " print(f'''\n",
    "        \n",
    "        Model: {knn}\n",
    "        Accuracy: {accuracy:.2%}\n",
    "        True Postive Rate: {tp_rate:.2%}\n",
    "        False Positive Rate: {fp_rate:.2%}\n",
    "        True Negative Rate: {tn_rate:.2%}\n",
    "        False Negative Rate: {fn_rate:.2%}\n",
    "        Precision: {precision_:.2%}\n",
    "        Recall: {recall:.2%}\n",
    "        F1: {f1:.2%}\n",
    "        Validation Set Classification Report:\n",
    "        \n",
    "        {classification_report(y_train, y_pred)}\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93559e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30a819c2",
   "metadata": {},
   "source": [
    "> #### 4. Run through steps 1-3 setting k to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b696fa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# create\n",
    "knn10 = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# fit\n",
    "knn10.fit(X_train,y_train )\n",
    "\n",
    "\n",
    "# prediction\n",
    "y_preds10 = knn10.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6cfc783",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_train, y_preds10).ravel()\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fn + fp)\n",
    "\n",
    "tp_rate =  tp / (tp + fn)\n",
    "\n",
    "fp_rate = fp / (fp + tn)\n",
    "\n",
    "tn_rate = tn / (tn + fp)\n",
    "\n",
    "fn_rate = fn / (tp + fn)\n",
    "\n",
    "precision_ = tp / (tp + fp)\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "f1 = 2 * (precision_ * recall) / (precision_ + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac00f18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       \n",
      "       Model: KNeighborsClassifier()\n",
      "       Accuracy: 72.09%\n",
      "       True Postive Rate: 42.41%\n",
      "       False Positive Rate: 9.45%\n",
      "       True Negative Rate: 90.55%\n",
      "       False Negative Rate: 57.59%\n",
      "       Precision: 73.64%\n",
      "       Recall: 42.41%\n",
      "       F1: 53.82%\n",
      "       Validation Set Classification Report:\n",
      "       \n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.91      0.82       307\n",
      "           1       0.77      0.52      0.62       191\n",
      "\n",
      "    accuracy                           0.76       498\n",
      "   macro avg       0.76      0.71      0.72       498\n",
      "weighted avg       0.76      0.76      0.74       498\n",
      "\n",
      "       \n"
     ]
    }
   ],
   "source": [
    " print(f'''\n",
    "        \n",
    "        Model: {knn}\n",
    "        Accuracy: {accuracy:.2%}\n",
    "        True Postive Rate: {tp_rate:.2%}\n",
    "        False Positive Rate: {fp_rate:.2%}\n",
    "        True Negative Rate: {tn_rate:.2%}\n",
    "        False Negative Rate: {fn_rate:.2%}\n",
    "        Precision: {precision_:.2%}\n",
    "        Recall: {recall:.2%}\n",
    "        F1: {f1:.2%}\n",
    "        Validation Set Classification Report:\n",
    "        \n",
    "        {classification_report(y_train, y_preds10)}\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c58212",
   "metadata": {},
   "source": [
    "> #### 5. Run through steps 1-3 setting k to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4beb89e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# create\n",
    "knn20 = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# fit\n",
    "knn20.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# prediction\n",
    "y_preds20 = knn20.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd047b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_train, y_preds20).ravel()\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fn + fp)\n",
    "\n",
    "tp_rate =  tp / (tp + fn)\n",
    "\n",
    "fp_rate = fp / (fp + tn)\n",
    "\n",
    "tn_rate = tn / (tn + fp)\n",
    "\n",
    "fn_rate = fn / (tp + fn)\n",
    "\n",
    "precision_ = tp / (tp + fp)\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "f1 = 2 * (precision_ * recall) / (precision_ + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "173f1899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       \n",
      "       Model: KNeighborsClassifier()\n",
      "       Accuracy: 72.09%\n",
      "       True Postive Rate: 42.41%\n",
      "       False Positive Rate: 9.45%\n",
      "       True Negative Rate: 90.55%\n",
      "       False Negative Rate: 57.59%\n",
      "       Precision: 73.64%\n",
      "       Recall: 42.41%\n",
      "       F1: 53.82%\n",
      "       Validation Set Classification Report:\n",
      "       \n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.91      0.80       307\n",
      "           1       0.74      0.42      0.54       191\n",
      "\n",
      "    accuracy                           0.72       498\n",
      "   macro avg       0.73      0.66      0.67       498\n",
      "weighted avg       0.72      0.72      0.70       498\n",
      "\n",
      "       \n"
     ]
    }
   ],
   "source": [
    " print(f'''\n",
    "        \n",
    "        Model: {knn}\n",
    "        Accuracy: {accuracy:.2%}\n",
    "        True Postive Rate: {tp_rate:.2%}\n",
    "        False Positive Rate: {fp_rate:.2%}\n",
    "        True Negative Rate: {tn_rate:.2%}\n",
    "        False Negative Rate: {fn_rate:.2%}\n",
    "        Precision: {precision_:.2%}\n",
    "        Recall: {recall:.2%}\n",
    "        F1: {f1:.2%}\n",
    "        Validation Set Classification Report:\n",
    "        \n",
    "        {classification_report(y_train, y_preds20)}\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539a368",
   "metadata": {},
   "source": [
    "> #### 6. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ad28961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "neighbors = [1, 10, 20]\n",
    "knn_dict = {}\n",
    "\n",
    "for i in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn\n",
    "    knn_dict[f'{i} neighbors'] = {'Train Score': round(knn.score(X_train, y_train), 2), 'Validate Score': round(knn.score(X_val, y_val), 2), 'Difference': round(knn.score(X_train, y_train) - knn.score(X_val, y_val), 2) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f69b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0.76\n",
      "0.72\n"
     ]
    }
   ],
   "source": [
    "for i in neighbors:\n",
    "    print(knn_dict[f'{i} neighbors']['Train Score'])\n",
    "    \n",
    "# the highest score for the first ds is the nearest neighbor of 1\n",
    "# Why? I'm not sure, maybe overfitting? becasue there is a huges difference on the validat eset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9b37e",
   "metadata": {},
   "source": [
    "> #### 7. Which model performs best on our out-of-sample data from validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bdbd79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n",
      "0.68\n",
      "0.67\n"
     ]
    }
   ],
   "source": [
    "for i in neighbors:\n",
    "    print(knn_dict[f'{i} neighbors']['Validate Score'])\n",
    "    \n",
    "# the model with 10 performs the best "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754719a",
   "metadata": {},
   "source": [
    "> ### 1. Fit a K-Nearest Neighbors classifier to your training sample and transform (i.e. make predictions on the training sample)1. Fit a K-Nearest Neighbors classifier to your training sample and transform (i.e. make predictions on the training sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73fb56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204cdd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92fab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f240ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
